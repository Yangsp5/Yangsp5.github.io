<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>bert | 我的学习小屋</title><meta name="keywords" content="bert"><meta name="author" content="yangsp5"><meta name="copyright" content="yangsp5"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="由于已经了解了transformer了 这篇博客记录下bert是怎么做预训练、fine tune，并以源码形式的展示 源码网址  数据的清洗 数据的清洗代表了，训练的方法 样本数据一共有三篇短文（以空行分隔），每一行为一个自然句子，下面展示两篇短文  读取文章，并转成unicode的形式  将句子分词，就是tokenize  取一篇文章，我们允许的句子最大长度是max_seq_length&#x3D;12">
<meta property="og:type" content="article">
<meta property="og:title" content="bert">
<meta property="og:url" content="http://yangsp5.github.io/2020/12/14/bert/index.html">
<meta property="og:site_name" content="我的学习小屋">
<meta property="og:description" content="由于已经了解了transformer了 这篇博客记录下bert是怎么做预训练、fine tune，并以源码形式的展示 源码网址  数据的清洗 数据的清洗代表了，训练的方法 样本数据一共有三篇短文（以空行分隔），每一行为一个自然句子，下面展示两篇短文  读取文章，并转成unicode的形式  将句子分词，就是tokenize  取一篇文章，我们允许的句子最大长度是max_seq_length&#x3D;12">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yangsp5.github.io/2020/12/14/bert/bert.jpg">
<meta property="article:published_time" content="2020-12-14T02:27:44.000Z">
<meta property="article:modified_time" content="2020-12-15T03:35:53.270Z">
<meta property="article:author" content="yangsp5">
<meta property="article:tag" content="bert">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yangsp5.github.io/2020/12/14/bert/bert.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yangsp5.github.io/2020/12/14/bert/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-12-15 11:35:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/katto.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2020/12/14/bert/bert.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">我的学习小屋</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">bert</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-12-14T02:27:44.000Z" title="Created 2020-12-14 10:27:44">2020-12-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-12-15T03:35:53.270Z" title="Updated 2020-12-15 11:35:53">2020-12-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/nlp/">nlp</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="bert"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><ul>
<li>由于已经了解了transformer了</li>
<li>这篇博客记录下bert是怎么做预训练、fine tune，并以源码形式的展示</li>
<li>源码<a target="_blank" rel="noopener" href="https://github.com/google-research/bert/blob/master/modeling.py">网址</a></li>
</ul>
<h1 id="数据的清洗"><a href="#数据的清洗" class="headerlink" title="数据的清洗"></a>数据的清洗</h1><ul>
<li>数据的清洗代表了，训练的方法</li>
<li><p>样本数据一共有三篇短文（以空行分隔），每一行为一个自然句子，下面展示两篇短文</p>
<ol>
<li><p>读取文章，并转成<code>unicode</code>的形式</p>
</li>
<li><p>将句子分词，就是tokenize</p>
</li>
<li><p>取一篇文章，我们允许的句子最大长度是<code>max_seq_length=128</code>，但每个句子会加上<code>[CLS] [SEP] [SEP]</code>，所以从文章的句子中抽取的句子长度是<code>max_num_tokens=128-125</code>，所以我们目标取出的<code>target_seq_length=max_num_tokens=125</code></p>
</li>
<li><p>注意，通常希望把序列的长度填充到最大长度，所以短的句子浪费计算消耗，但有时候（大概10%的时候）希望采用短句来最小化预训练和微调的差异，所以源码中有<code>short_seq_prob=0.1</code>，以这个概率生成短句子，就是<code>target_seq_length=[2, ..., 125]</code></p>
</li>
<li><p>源码中维护了一个<code>chunk</code>，用了不断的添加句子进去，直到这个<code>chunk</code>的token数量超过<code>target_seq_length</code>或取完文章中的所有句子了</p>
</li>
<li><p>当<code>chunk</code>满足条件后，开始真正的处理两个task</p>
</li>
</ol>
</li>
<li><p>这里就有<code>next prediction task</code>了，以50%的概率，决定下一个句子是不是真实的下一个句子。如果不是，就随机从文章数据集中随便抽一个序列出来，替换掉<code>chunk</code>中句子B的tokens。</p>
</li>
<li>上面经过<code>next prediction task</code>处理过的tokens后，还要经过随机掩码的处理<ul>
<li>取tokens中非<code>[CLS] [SEP]</code>的下标集，作为备选</li>
<li>随机抽15%个token进行掩码，这里设置了<code>max_predictions_per_seq=20</code>，意思是最少也要抽20个tokens进行掩码<ul>
<li>抽出来的token，以80%的概率，换成<code>[MASK]</code></li>
<li>抽出来的token，以10%的概率，保持不变</li>
<li>抽出来的token，以10%的概率，在词表中随机取一个token出来替换</li>
</ul>
</li>
</ul>
</li>
<li>源码中还有一个<code>dupe_factor=10</code>的参数，意思是重复上述过程10次，那么每次产生的掩码位置不一样，就可以重复利用句子的信息了</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">This text is included to make sure Unicode is handled properly: 力加勝北区ᴵᴺᵀᵃছজটডণত</span><br><span class="line">Text should be one-sentence-per-line, with empty lines between documents.</span><br><span class="line">This sample text is public domain and was randomly selected from Project Guttenberg.</span><br><span class="line"></span><br><span class="line">The rain had only ceased with the gray streaks of morning at Blazing Star, and the settlement awoke to a moral sense of cleanliness, and the finding of forgotten knives, tin cups, and smaller camp utensils, where the heavy showers had washed away the debris and dust heaps before the cabin doors.</span><br><span class="line">Indeed, it was recorded in Blazing Star that a fortunate early riser had once picked up on the highway a solid chunk of gold quartz which the rain had freed from its incumbering soil, and washed into immediate and glittering popularity.</span><br><span class="line">Possibly this may have been the reason why early risers in that locality, during the rainy season, adopted a thoughtful habit of body, and seldom lifted their eyes to the rifted or india-ink washed skies above them.</span><br><span class="line">&quot;Cass&quot; Beard had risen early that morning, but not with a view to discovery.</span><br><span class="line">A leak in his cabin roof,--quite consistent with his careless, improvident habits,--had roused him at 4 A. M., with a flooded &quot;bunk&quot; and wet blankets.</span><br><span class="line">The chips from his wood pile refused to kindle a fire to dry his bed-clothes, and he had recourse to a more provident neighbor&#39;s to supply the deficiency.</span><br><span class="line">This was nearly opposite.</span><br><span class="line">Mr. Cassius crossed the highway, and stopped suddenly.</span><br><span class="line">Something glittered in the nearest red pool before him.</span><br><span class="line">Gold, surely!</span><br><span class="line">But, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from Nature&#39;s crucible, but a bit of jeweler&#39;s handicraft in the form of a plain gold ring.</span><br><span class="line">Looking at it more attentively, he saw that it bore the inscription, &quot;May to Cass.&quot;</span><br><span class="line">Like most of his fellow gold-seekers, Cass was superstitious.</span><br></pre></td></tr></table></figure>
<ul>
<li>下面只展示<code>create_pretraining_data.py</code>的部分代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_instances</span>(<span class="params">input_files, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                              dupe_factor, short_seq_prob, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                              max_predictions_per_seq, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Create `TrainingInstance`s from raw text.&quot;&quot;&quot;</span></span><br><span class="line">  all_documents = [[]]</span><br><span class="line">  <span class="comment"># 输入的格式：一行一句话，是自然意义上的句子，不是整个段落</span></span><br><span class="line">  <span class="comment"># 因为要用到next sentence prediction task</span></span><br><span class="line">  <span class="comment"># 文章之间是空行</span></span><br><span class="line">  <span class="comment"># next sentence prediction 不会在文章之间</span></span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(input_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 读取每一行，并转成unicode的形式</span></span><br><span class="line">        line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 去掉收尾的空格</span></span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># 空行被用作文章的分割符</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          all_documents.append([])</span><br><span class="line">        <span class="comment"># 分词，将词语切分成一个一个token</span></span><br><span class="line">        tokens = tokenizer.tokenize(line)</span><br><span class="line">        <span class="keyword">if</span> tokens:</span><br><span class="line">          all_documents[-<span class="number">1</span>].append(tokens)</span><br><span class="line">  <span class="comment"># 去掉空的文章</span></span><br><span class="line">  all_documents = [x <span class="keyword">for</span> x <span class="keyword">in</span> all_documents <span class="keyword">if</span> x]</span><br><span class="line">  rng.shuffle(all_documents) <span class="comment"># 打乱文章</span></span><br><span class="line"></span><br><span class="line">  vocab_words = <span class="built_in">list</span>(tokenizer.vocab.keys())</span><br><span class="line">  instances = []</span><br><span class="line">  <span class="comment">#dupe_factor=10, 复制输入数据的次数, 相同的数据形成不同的掩码, 增加实例用于训练</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(dupe_factor):</span><br><span class="line">    <span class="keyword">for</span> document_index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(all_documents)):</span><br><span class="line">      <span class="comment"># 取一篇文章</span></span><br><span class="line">      <span class="comment"># max_seq_length=128</span></span><br><span class="line">      instances.extend(</span><br><span class="line">          create_instances_from_document(</span><br><span class="line">              all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))</span><br><span class="line"></span><br><span class="line">  rng.shuffle(instances)</span><br><span class="line">  <span class="keyword">return</span> instances</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_instances_from_document</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    all_documents, document_index, max_seq_length, short_seq_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">    masked_lm_prob, max_predictions_per_seq, vocab_words, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates `TrainingInstance`s for a single document.&quot;&quot;&quot;</span></span><br><span class="line">  document = all_documents[document_index]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 序列需要加入 [CLS], [SEP], [SEP]</span></span><br><span class="line">  <span class="comment"># max_num_tokens=125</span></span><br><span class="line">  max_num_tokens = max_seq_length - <span class="number">3</span></span><br><span class="line">  <span class="comment"># 通常希望把序列的长度填充到最大长度，所以短的句子浪费计算消耗</span></span><br><span class="line">  <span class="comment"># 有时候（大概10%的时候）希望采用短句来最小化预训练和微调的差异</span></span><br><span class="line">  <span class="comment"># target_seq_length 是一个粗略的目标，而 max_seq_length是一个强制的限制。</span></span><br><span class="line">  target_seq_length = max_num_tokens</span><br><span class="line">  <span class="keyword">if</span> rng.random() &lt; short_seq_prob: <span class="comment"># short_seq_prob=0.1，也就是需要短句子的时候</span></span><br><span class="line">    target_seq_length = rng.randint(<span class="number">2</span>, max_num_tokens)</span><br><span class="line">  <span class="comment"># 这里并没有把文章的所有token合并成一个序列</span></span><br><span class="line">  <span class="comment"># 然后取125个token序列，并随机取一个分割点</span></span><br><span class="line">  <span class="comment"># 而是，基于现实中的句子，把句子分割成真实的A和B</span></span><br><span class="line">  instances = []</span><br><span class="line">  current_chunk = []</span><br><span class="line">  current_length = <span class="number">0</span></span><br><span class="line">  i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(document): <span class="comment"># len(document)是文章的行数</span></span><br><span class="line">    segment = document[i]</span><br><span class="line">    <span class="comment"># 把随机打乱后的第document_index篇文章的第i行，加入到chunk中</span></span><br><span class="line">    current_chunk.append(segment)</span><br><span class="line">    current_length += <span class="built_in">len</span>(segment)</span><br><span class="line">    <span class="comment"># 当取到最后一个句子 或 chunk里面的句子长度大于target_seq_length时</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="built_in">len</span>(document) - <span class="number">1</span> <span class="keyword">or</span> current_length &gt;= target_seq_length:</span><br><span class="line">      <span class="keyword">if</span> current_chunk:</span><br><span class="line">        <span class="comment"># `a_end` is how many segments from `current_chunk` go into the `A`</span></span><br><span class="line">        <span class="comment"># (first) sentence.</span></span><br><span class="line">        a_end = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(current_chunk) &gt;= <span class="number">2</span>:</span><br><span class="line">          a_end = rng.randint(<span class="number">1</span>, <span class="built_in">len</span>(current_chunk) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tokens_a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(a_end):</span><br><span class="line">          tokens_a.extend(current_chunk[j])</span><br><span class="line"></span><br><span class="line">        tokens_b = []</span><br><span class="line">        <span class="comment"># 50%的概率，下一个句子不是真实的句子</span></span><br><span class="line">        is_random_next = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(current_chunk) == <span class="number">1</span> <span class="keyword">or</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          is_random_next = <span class="literal">True</span></span><br><span class="line">          target_b_length = target_seq_length - <span class="built_in">len</span>(tokens_a)</span><br><span class="line">          <span class="comment"># 对于大的语料库，这个迭代会很慢</span></span><br><span class="line">          <span class="comment"># 这里，保证了随机取得文章不是现在正在处理的文章</span></span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            random_document_index = rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(all_documents) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> random_document_index != document_index:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line">          <span class="comment"># 随机取一篇文章</span></span><br><span class="line">          random_document = all_documents[random_document_index]</span><br><span class="line">          <span class="comment"># 随机从文章的任意位置开始</span></span><br><span class="line">          random_start = rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(random_document) - <span class="number">1</span>)</span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(random_start, <span class="built_in">len</span>(random_document)):</span><br><span class="line">            tokens_b.extend(random_document[j])</span><br><span class="line">            <span class="comment"># 补齐之后，结束</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(tokens_b) &gt;= target_b_length:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line">          <span class="comment"># 不要浪费没有使用过的文章中的行数</span></span><br><span class="line">          num_unused_segments = <span class="built_in">len</span>(current_chunk) - a_end</span><br><span class="line">          i -= num_unused_segments</span><br><span class="line">        <span class="comment"># 50%的概率，下一个句子是真实的句子</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          is_random_next = <span class="literal">False</span></span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(a_end, <span class="built_in">len</span>(current_chunk)):</span><br><span class="line">            tokens_b.extend(current_chunk[j])</span><br><span class="line">        <span class="comment"># 生成A和B序列后</span></span><br><span class="line">        <span class="comment"># 截断两个序列，以保证两个序列之和不超过max_num_tokens</span></span><br><span class="line">        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(tokens_a) &gt;= <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(tokens_b) &gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        tokens = []</span><br><span class="line">        segment_ids = []</span><br><span class="line">        tokens.append(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_a:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        tokens.append(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_b:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">1</span>)</span><br><span class="line">        tokens.append(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">        segment_ids.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这里，tokens是已经被掩码盖住过的tokens序列</span></span><br><span class="line">        <span class="comment"># masked_lm_positions是掩码的位置</span></span><br><span class="line">        <span class="comment"># masked_lm_labels是被掩码的token的真实值</span></span><br><span class="line">        (tokens, masked_lm_positions,</span><br><span class="line">         masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br><span class="line">        <span class="comment"># 生成序列</span></span><br><span class="line">        instance = TrainingInstance(</span><br><span class="line">            tokens=tokens,</span><br><span class="line">            segment_ids=segment_ids,</span><br><span class="line">            is_random_next=is_random_next,</span><br><span class="line">            masked_lm_positions=masked_lm_positions,</span><br><span class="line">            masked_lm_labels=masked_lm_labels)</span><br><span class="line">        instances.append(instance)</span><br><span class="line">      current_chunk = []</span><br><span class="line">      current_length = <span class="number">0</span></span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> instances</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span>(<span class="params">tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates the predictions for the masked LM objective.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># 选出tokens中，可能被掩码的位置</span></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&quot;[CLS]&quot;</span> <span class="keyword">or</span> token == <span class="string">&quot;[SEP]&quot;</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    cand_indexes.append(i)</span><br><span class="line"></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line">  <span class="comment"># 这里tokens本来就是list了，list(tokens)是什么意思</span></span><br><span class="line">  output_tokens = <span class="built_in">list</span>(tokens)</span><br><span class="line">  <span class="comment"># collections.namedtuple(&#x27;名称&#x27;, [属性list])，给tuple命名</span></span><br><span class="line">  masked_lm = collections.namedtuple(<span class="string">&quot;masked_lm&quot;</span>, [<span class="string">&quot;index&quot;</span>, <span class="string">&quot;label&quot;</span>])  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">  <span class="comment"># max_predictions_per_seq=20</span></span><br><span class="line">  <span class="comment"># max(1, int(round(len(tokens) * masked_lm_prob)))是大概有多少个tokens要被掩码</span></span><br><span class="line">  num_to_predict = <span class="built_in">min</span>(max_predictions_per_seq,</span><br><span class="line">                       <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">int</span>(<span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = <span class="built_in">set</span>()</span><br><span class="line">  <span class="keyword">for</span> index <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="comment"># 掩码的token个数已经够了</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">      <span class="comment"># 这个token已经被掩码了</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">    masked_token = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">    <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">      masked_token = <span class="string">&quot;[MASK]&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        masked_token = tokens[index]</span><br><span class="line">      <span class="comment"># 10% of the time, 在词表中随机抽一个token出来</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        masked_token = vocab_words[rng.randint(<span class="number">0</span>, <span class="built_in">len</span>(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">    masked_lms.append(masked_lm(index=index, label=tokens[index]))</span><br><span class="line"></span><br><span class="line">  masked_lms = <span class="built_in">sorted</span>(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_seq_pair</span>(<span class="params">tokens_a, tokens_b, max_num_tokens, rng</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Truncates a pair of sequences to a maximum sequence length.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    total_length = <span class="built_in">len</span>(tokens_a) + <span class="built_in">len</span>(tokens_b)</span><br><span class="line">    <span class="keyword">if</span> total_length &lt;= max_num_tokens:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    trunc_tokens = tokens_a <span class="keyword">if</span> <span class="built_in">len</span>(tokens_a) &gt; <span class="built_in">len</span>(tokens_b) <span class="keyword">else</span> tokens_b</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(trunc_tokens) &gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机从最前面、最后面截断</span></span><br><span class="line">    <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">      <span class="keyword">del</span> trunc_tokens[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      trunc_tokens.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">_</span>):</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  tokenizer = tokenization.FullTokenizer(</span><br><span class="line">      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">&quot;,&quot;</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">&quot;*** Reading from input files ***&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  %s&quot;</span>, input_file)</span><br><span class="line"></span><br><span class="line">  rng = random.Random(FLAGS.random_seed)</span><br><span class="line">  instances = create_training_instances(</span><br><span class="line">      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,</span><br><span class="line">      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,</span><br><span class="line">      rng)</span><br><span class="line"></span><br><span class="line">  output_files = FLAGS.output_file.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">  tf.logging.info(<span class="string">&quot;*** Writing to output files ***&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  %s&quot;</span>, output_file)</span><br><span class="line">  <span class="comment"># 把instances写到TF中</span></span><br><span class="line">  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,</span><br><span class="line">                                  FLAGS.max_predictions_per_seq, output_files)</span><br></pre></td></tr></table></figure>
<h1 id="Bert的结构"><a href="#Bert的结构" class="headerlink" title="Bert的结构"></a>Bert的结构</h1><ul>
<li>bert与transformer那篇文章结构不同的地方在于<ul>
<li>bert只使用了encoder的结构，没有decoder</li>
<li>模型维度不一样</li>
<li>激活函数改成了<code>gelu</code>，定义和近似的计算方法如下</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
\operatorname{GELU}(x) &=x P(X<=x)=x \Phi(x) \\
G E L U(x) &=0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left(x+0.044715 x^{3}\right)\right]\right)
\end{align}</script><ul>
<li>基础的超参数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里设置了bert的模型参数</span></span><br><span class="line"><span class="comment"># 这里把原来的参数改了 bert chinese的参数为例</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertConfig</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Configuration for `BertModel`. 这个类生成了bert的config&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 下面是bert chinese的config</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;attention_probs_dropout_prob&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">  <span class="string">&quot;directionality&quot;</span>: <span class="string">&quot;bidi&quot;</span>, </span><br><span class="line">  <span class="string">&quot;hidden_act&quot;</span>: <span class="string">&quot;gelu&quot;</span>,  <span class="comment"># 激活函数，transformer里面是relu</span></span><br><span class="line">  <span class="string">&quot;hidden_dropout_prob&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">  <span class="string">&quot;hidden_size&quot;</span>: <span class="number">768</span>, <span class="comment"># 模型的维度transformer里面是512</span></span><br><span class="line">  <span class="string">&quot;initializer_range&quot;</span>: <span class="number">0.02</span>, </span><br><span class="line">  <span class="string">&quot;intermediate_size&quot;</span>: <span class="number">3072</span>,  <span class="comment"># encoder的“中间”隐层神经元数量 如前馈神经网络，在transformer里面是2048</span></span><br><span class="line">  <span class="string">&quot;max_position_embeddings&quot;</span>: <span class="number">512</span>, <span class="comment"># 最大的位置变量长度？？</span></span><br><span class="line">  <span class="string">&quot;num_attention_heads&quot;</span>: <span class="number">12</span>,  <span class="comment"># 多头注意力的个数，注意hidden_size一定整除heads</span></span><br><span class="line">  <span class="string">&quot;num_hidden_layers&quot;</span>: <span class="number">12</span>,  <span class="comment"># encoder的数量</span></span><br><span class="line">  <span class="string">&quot;pooler_fc_size&quot;</span>: <span class="number">768</span>, </span><br><span class="line">  <span class="string">&quot;pooler_num_attention_heads&quot;</span>: <span class="number">12</span>, </span><br><span class="line">  <span class="string">&quot;pooler_num_fc_layers&quot;</span>: <span class="number">3</span>, </span><br><span class="line">  <span class="string">&quot;pooler_size_per_head&quot;</span>: <span class="number">128</span>, </span><br><span class="line">  <span class="string">&quot;pooler_type&quot;</span>: <span class="string">&quot;first_token_transform&quot;</span>, </span><br><span class="line">  <span class="string">&quot;type_vocab_size&quot;</span>: <span class="number">2</span>, <span class="comment"># token_type_ids的词典大小</span></span><br><span class="line">  <span class="string">&quot;vocab_size&quot;</span>: <span class="number">21128</span>  <span class="comment"># 词表大小</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Bert模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">&quot;bert&quot;</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embeddings&quot;</span>):</span><br><span class="line">        <span class="comment"># 这里生成一个 vocab_size * hidden_size的二位矩阵</span></span><br><span class="line">        <span class="comment"># 每一行都是一个token的词向量</span></span><br><span class="line">        <span class="comment"># use_one_hot_embeddings是指，矩阵索引取值还是矩阵乘法取值，结果都一样，速度快慢不一样</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids, <span class="comment"># 输入序列中每个字的idx表示</span></span><br><span class="line">            vocab_size=config.vocab_size, <span class="comment"># 21128</span></span><br><span class="line">            embedding_size=config.hidden_size, <span class="comment"># 768</span></span><br><span class="line">            initializer_range=config.initializer_range,<span class="comment">#初始化参数</span></span><br><span class="line">            word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 词向量的后续处理，添加 位置向量 句子分割向量</span></span><br><span class="line">        <span class="comment"># token_type_embedding 就是 2*768 的矩阵</span></span><br><span class="line">        <span class="comment"># position_embedding 是 512*768 的矩阵</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output, <span class="comment"># 这个是上面的</span></span><br><span class="line">            use_token_type=<span class="literal">True</span>, <span class="comment"># 是否要分割句子</span></span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size, <span class="comment"># 2</span></span><br><span class="line">            token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line">        </span><br><span class="line">	  <span class="comment"># 构造self-attention的mask矩阵</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line">        <span class="comment"># 输入[batch_size, seq_length]的idx和掩码的位置</span></span><br><span class="line">        <span class="comment"># 返回[batch_size, seq_length, seq_length]的掩码矩阵</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)</span><br><span class="line">        <span class="comment"># transformer层，实际上是12个encoder层</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output, <span class="comment"># [batch_size, seq_length, hidden_size]</span></span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size, <span class="comment"># 768</span></span><br><span class="line">            num_hidden_layers=config.num_hidden_layers, <span class="comment"># 12</span></span><br><span class="line">            num_attention_heads=config.num_attention_heads, <span class="comment"># 12</span></span><br><span class="line">            intermediate_size=config.intermediate_size, <span class="comment"># 3072</span></span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act), <span class="comment"># gelu</span></span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>) <span class="comment"># 返回12层所有的值</span></span><br><span class="line">	  <span class="comment"># 最后的输出是，第12层的输出值</span></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[-<span class="number">1</span>]</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># The &quot;pooler&quot; converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;pooler&quot;</span>):</span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure>
<ul>
<li>上面出现了<code>transformer_model</code>这个函数，函数代码如下</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span>(<span class="params">input_tensor, <span class="comment"># 进来的是经过词向量处理的Tensor [batch_size, seq_length, hidden_size]</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=<span class="literal">None</span>, <span class="comment"># attention_mask</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=<span class="literal">False</span></span>):</span> <span class="comment"># True</span></span><br><span class="line">  <span class="comment"># hidden size 一定要被 heads 整除</span></span><br><span class="line">  <span class="comment"># attention_head_size=64，就是transformer里面的d_x d_k d_v</span></span><br><span class="line">  attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>] <span class="comment"># 768</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># reshape操作在GPU上很快，在TPU上很慢</span></span><br><span class="line">  <span class="comment"># 所以为了避免2D和3D之间的频繁reshape，把所有的3D张量用2D矩阵表示</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor) <span class="comment"># [(batch_size*seq_length), 768]</span></span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layers): <span class="comment"># 12层</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;layer_%d&quot;</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output <span class="comment"># 上一层的输入值</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;attention&quot;</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self&quot;</span>):</span><br><span class="line">          <span class="comment"># 计算12个多头注意力矩阵</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads, <span class="comment"># 12</span></span><br><span class="line">              size_per_head=attention_head_size, <span class="comment"># 12</span></span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># 把12个heads拼接成一起， [(batch_size*seq_length), (64*12=768)]</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 投影层，实际上就是 transformer 的 w^&#123;o&#125; 矩阵 </span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          <span class="comment"># 残差层</span></span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">        </span><br><span class="line">      <span class="comment"># 前馈神经网络</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;intermediate&quot;</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size, <span class="comment"># 3072，transformers是2048</span></span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># 残差层</span></span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<h1 id="Bert的预训练"><a href="#Bert的预训练" class="headerlink" title="Bert的预训练"></a>Bert的预训练</h1><ul>
<li>源代码如下，这里讲讲思路</li>
<li>将已经清洗好的数据，放到模型中后</li>
<li>取出最后一个encoder的输出值，即<code>[batch_size, seq_length, hidden_size]</code></li>
<li>预测掩码的方法就是，对上述的值，取掩码对应的位置，即<code>[batch_size, 掩码位置（如1,3,6,11等）, hidden_size]</code>，再变成<code>[batch_size*掩码个数, hidden_size]</code>的形式，与embedding矩阵相乘（意思就是求相似度），得到预测的token的概率</li>
<li>预测next sententce 的方法就是，取上述的值，第一个token的位置，即<code>[batch_size, 0, hidden_size]</code></li>
<li>取出预测值之后，将预测值分别与真实值（要转成ont-hot）进行<code>tf.reduce_sum</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span>(<span class="params">bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Returns `model_fn` closure for TPUEstimator.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span>(<span class="params">features, labels, mode, params</span>):</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The `model_fn` for TPUEstimator.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">&quot;*** Features ***&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> <span class="built_in">sorted</span>(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">&quot;  name = %s, shape = %s&quot;</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    input_mask = features[<span class="string">&quot;input_mask&quot;</span>]</span><br><span class="line">    segment_ids = features[<span class="string">&quot;segment_ids&quot;</span>]</span><br><span class="line">    masked_lm_positions = features[<span class="string">&quot;masked_lm_positions&quot;</span>]</span><br><span class="line">    masked_lm_ids = features[<span class="string">&quot;masked_lm_ids&quot;</span>]</span><br><span class="line">    masked_lm_weights = features[<span class="string">&quot;masked_lm_weights&quot;</span>]</span><br><span class="line">    next_sentence_labels = features[<span class="string">&quot;next_sentence_labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line">    <span class="comment"># model.get_sequence_output() [batch_size, seq_length, hidden_size]</span></span><br><span class="line">    <span class="comment"># model.get_embedding_table() [vocab_size, hidden_size]</span></span><br><span class="line">    <span class="comment"># masked_lm_weights全是1</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model.get_pooled_output() 是序列的第一个token的预测值</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span>():</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">&quot;**** Trainable Variables ****&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">&quot;, *INIT_FROM_CKPT*&quot;</span></span><br><span class="line">      tf.logging.info(<span class="string">&quot;  name = %s, shape = %s%s&quot;</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span>(<span class="params">masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Computes the loss and accuracy of the model.&quot;&quot;&quot;</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [-<span class="number">1</span>, masked_lm_log_probs.shape[-<span class="number">1</span>]])</span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=-<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-<span class="number">1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [-<span class="number">1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [-<span class="number">1</span>])</span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [-<span class="number">1</span>, next_sentence_log_probs.shape[-<span class="number">1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=-<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [-<span class="number">1</span>])</span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;masked_lm_accuracy&quot;</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">&quot;masked_lm_loss&quot;</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">&quot;next_sentence_accuracy&quot;</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">&quot;next_sentence_loss&quot;</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;Only TRAIN and EVAL modes are supported: %s&quot;</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span>(<span class="params">bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line">  <span class="comment"># 这里的output_weights就是model.get_embedding_table()</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;cls/predictions&quot;</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;transform&quot;</span>):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor, <span class="comment"># [batch_size*掩码个数, hidden_size]</span></span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_bias&quot;</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    label_ids = tf.reshape(label_ids, [-<span class="number">1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-<span class="number">1</span>])</span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span>(<span class="params">bert_config, input_tensor, labels</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Get loss and log probs for the next sentence prediction.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is &quot;next sentence&quot; and 1 is</span></span><br><span class="line">  <span class="comment"># &quot;random sentence&quot;. This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;cls/seq_relationship&quot;</span>):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_weights&quot;</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">&quot;output_bias&quot;</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    labels = tf.reshape(labels, [-<span class="number">1</span>])</span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-<span class="number">1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather_indexes</span>(<span class="params">sequence_tensor, positions</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Gathers the vectors at the specific positions over a minibatch.&quot;&quot;&quot;</span></span><br><span class="line">  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = sequence_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = sequence_shape[<span class="number">1</span>]</span><br><span class="line">  width = sequence_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  flat_offsets = tf.reshape(</span><br><span class="line">      tf.<span class="built_in">range</span>(<span class="number">0</span>, batch_size, dtype=tf.int32) * seq_length, [-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">  flat_positions = tf.reshape(positions + flat_offsets, [-<span class="number">1</span>])</span><br><span class="line">  flat_sequence_tensor = tf.reshape(sequence_tensor,</span><br><span class="line">                                    [batch_size * seq_length, width])</span><br><span class="line">  <span class="comment"># tf.gather(params,indices,axis=0 )</span></span><br><span class="line">  <span class="comment"># 从params的axis维根据indices的参数值获取切片</span></span><br><span class="line">  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn_builder</span>(<span class="params">input_files,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_predictions_per_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">                     is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_cpu_threads=<span class="number">4</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span>(<span class="params">params</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The actual input function.&quot;&quot;&quot;</span></span><br><span class="line">    batch_size = params[<span class="string">&quot;batch_size&quot;</span>]</span><br><span class="line"></span><br><span class="line">    name_to_features = &#123;</span><br><span class="line">        <span class="string">&quot;input_ids&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">&quot;input_mask&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">&quot;segment_ids&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">&quot;masked_lm_positions&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">&quot;masked_lm_ids&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">&quot;masked_lm_weights&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),</span><br><span class="line">        <span class="string">&quot;next_sentence_labels&quot;</span>:</span><br><span class="line">            tf.FixedLenFeature([<span class="number">1</span>], tf.int64),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For training, we want a lot of parallel reading and shuffling.</span></span><br><span class="line">    <span class="comment"># For eval, we want no shuffling and parallel reading doesn&#x27;t matter.</span></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))</span><br><span class="line">      d = d.repeat()</span><br><span class="line">      d = d.shuffle(buffer_size=<span class="built_in">len</span>(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `cycle_length` is the number of parallel files that get read.</span></span><br><span class="line">      cycle_length = <span class="built_in">min</span>(num_cpu_threads, <span class="built_in">len</span>(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `sloppy` mode means that the interleaving is not exact. This adds</span></span><br><span class="line">      <span class="comment"># even more randomness to the training pipeline.</span></span><br><span class="line">      d = d.apply(</span><br><span class="line">          tf.contrib.data.parallel_interleave(</span><br><span class="line">              tf.data.TFRecordDataset,</span><br><span class="line">              sloppy=is_training,</span><br><span class="line">              cycle_length=cycle_length))</span><br><span class="line">      d = d.shuffle(buffer_size=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      d = tf.data.TFRecordDataset(input_files)</span><br><span class="line">      <span class="comment"># Since we evaluate for a fixed number of steps we don&#x27;t want to encounter</span></span><br><span class="line">      <span class="comment"># out-of-range exceptions.</span></span><br><span class="line">      d = d.repeat()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We must `drop_remainder` on training because the TPU requires fixed</span></span><br><span class="line">    <span class="comment"># size dimensions. For eval, we assume we are evaluating on the CPU or GPU</span></span><br><span class="line">    <span class="comment"># and we *don&#x27;t* want to drop the remainder, otherwise we wont cover</span></span><br><span class="line">    <span class="comment"># every sample.</span></span><br><span class="line">    d = d.apply(</span><br><span class="line">        tf.contrib.data.map_and_batch(</span><br><span class="line">            <span class="keyword">lambda</span> record: _decode_record(record, name_to_features),</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            num_parallel_batches=num_cpu_threads,</span><br><span class="line">            drop_remainder=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> input_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_decode_record</span>(<span class="params">record, name_to_features</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Decodes a record to a TensorFlow example.&quot;&quot;&quot;</span></span><br><span class="line">  example = tf.parse_single_example(record, name_to_features)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># tf.Example only supports tf.int64, but the TPU only supports tf.int32.</span></span><br><span class="line">  <span class="comment"># So cast all int64 to int32.</span></span><br><span class="line">  <span class="keyword">for</span> name <span class="keyword">in</span> <span class="built_in">list</span>(example.keys()):</span><br><span class="line">    t = example[name]</span><br><span class="line">    <span class="keyword">if</span> t.dtype == tf.int64:</span><br><span class="line">      t = tf.to_int32(t)</span><br><span class="line">    example[name] = t</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> example</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">_</span>):</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.do_train <span class="keyword">and</span> <span class="keyword">not</span> FLAGS.do_eval:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;At least one of `do_train` or `do_eval` must be True.&quot;</span>)</span><br><span class="line">  <span class="comment"># 读取bert模型的超参数</span></span><br><span class="line">  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)</span><br><span class="line"></span><br><span class="line">  tf.gfile.MakeDirs(FLAGS.output_dir)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">&quot;,&quot;</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">&quot;*** Input Files ***&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  %s&quot;</span> % input_file)</span><br><span class="line"></span><br><span class="line">  tpu_cluster_resolver = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">if</span> FLAGS.use_tpu <span class="keyword">and</span> FLAGS.tpu_name:</span><br><span class="line">    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(</span><br><span class="line">        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br><span class="line"></span><br><span class="line">  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2</span><br><span class="line">  run_config = tf.contrib.tpu.RunConfig(</span><br><span class="line">      cluster=tpu_cluster_resolver,</span><br><span class="line">      master=FLAGS.master,</span><br><span class="line">      model_dir=FLAGS.output_dir,</span><br><span class="line">      save_checkpoints_steps=FLAGS.save_checkpoints_steps,</span><br><span class="line">      tpu_config=tf.contrib.tpu.TPUConfig(</span><br><span class="line">          iterations_per_loop=FLAGS.iterations_per_loop,</span><br><span class="line">          num_shards=FLAGS.num_tpu_cores,</span><br><span class="line">          per_host_input_for_training=is_per_host))</span><br><span class="line"></span><br><span class="line">  model_fn = model_fn_builder(</span><br><span class="line">      bert_config=bert_config,</span><br><span class="line">      init_checkpoint=FLAGS.init_checkpoint,</span><br><span class="line">      learning_rate=FLAGS.learning_rate,</span><br><span class="line">      num_train_steps=FLAGS.num_train_steps,</span><br><span class="line">      num_warmup_steps=FLAGS.num_warmup_steps,</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      use_one_hot_embeddings=FLAGS.use_tpu)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># If TPU is not available, this will fall back to normal Estimator on CPU</span></span><br><span class="line">  <span class="comment"># or GPU.</span></span><br><span class="line">  estimator = tf.contrib.tpu.TPUEstimator(</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      model_fn=model_fn,</span><br><span class="line">      config=run_config,</span><br><span class="line">      train_batch_size=FLAGS.train_batch_size,</span><br><span class="line">      eval_batch_size=FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  Batch size = %d&quot;</span>, FLAGS.train_batch_size)</span><br><span class="line">    train_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">True</span>)</span><br><span class="line">    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_eval:</span><br><span class="line">    tf.logging.info(<span class="string">&quot;***** Running evaluation *****&quot;</span>)</span><br><span class="line">    tf.logging.info(<span class="string">&quot;  Batch size = %d&quot;</span>, FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line">    eval_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    result = estimator.evaluate(</span><br><span class="line">        input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)</span><br><span class="line"></span><br><span class="line">    output_eval_file = os.path.join(FLAGS.output_dir, <span class="string">&quot;eval_results.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(output_eval_file, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> writer:</span><br><span class="line">      tf.logging.info(<span class="string">&quot;***** Eval results *****&quot;</span>)</span><br><span class="line">      <span class="keyword">for</span> key <span class="keyword">in</span> <span class="built_in">sorted</span>(result.keys()):</span><br><span class="line">        tf.logging.info(<span class="string">&quot;  %s = %s&quot;</span>, key, <span class="built_in">str</span>(result[key]))</span><br><span class="line">        writer.write(<span class="string">&quot;%s = %s\n&quot;</span> % (key, <span class="built_in">str</span>(result[key])))</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/bert/">bert</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/23/loss%E4%BC%98%E5%8C%96/"><img class="prev-cover" src="/img/top_img.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">loss优化</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/10/dropout/"><img class="next-cover" src="/2020/12/10/dropout/dropout_net.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">dropout</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/katto.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">yangsp5</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yangsp5"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E6%B8%85%E6%B4%97"><span class="toc-number">1.</span> <span class="toc-text">数据的清洗</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bert%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">Bert的结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bert%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.</span> <span class="toc-text">Bert的预训练</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/2020/12/14/bert/bert.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By yangsp5</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>